llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:        CPU  output buffer size =     0.98 MiB
create_memory: n_ctx = 512 (padded)
llama_kv_cache_unified: kv_size = 512, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32
llama_kv_cache_unified: layer   0: dev = CPU
llama_kv_cache_unified: layer   1: dev = CPU
llama_kv_cache_unified: layer   2: dev = CPU
llama_kv_cache_unified: layer   3: dev = CPU
llama_kv_cache_unified: layer   4: dev = CPU
llama_kv_cache_unified: layer   5: dev = CPU
llama_kv_cache_unified: layer   6: dev = CPU
llama_kv_cache_unified: layer   7: dev = CPU
llama_kv_cache_unified: layer   8: dev = CPU
llama_kv_cache_unified: layer   9: dev = CPU
llama_kv_cache_unified: layer  10: dev = CPU
llama_kv_cache_unified: layer  11: dev = CPU
llama_kv_cache_unified: layer  12: dev = CPU
llama_kv_cache_unified: layer  13: dev = CPU
llama_kv_cache_unified: layer  14: dev = CPU
llama_kv_cache_unified: layer  15: dev = CPU
llama_kv_cache_unified: layer  16: dev = CPU
llama_kv_cache_unified: layer  17: dev = CPU
llama_kv_cache_unified: layer  18: dev = CPU
llama_kv_cache_unified: layer  19: dev = CPU
llama_kv_cache_unified: layer  20: dev = CPU
llama_kv_cache_unified: layer  21: dev = CPU
llama_kv_cache_unified: layer  22: dev = CPU
llama_kv_cache_unified: layer  23: dev = CPU
llama_kv_cache_unified: layer  24: dev = CPU
llama_kv_cache_unified: layer  25: dev = CPU
llama_kv_cache_unified: layer  26: dev = CPU
llama_kv_cache_unified: layer  27: dev = CPU
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 1
llama_context: max_nodes = 65536
llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0
llama_context: reserving graph for n_tokens = 512, n_seqs = 1
llama_context: reserving graph for n_tokens = 1, n_seqs = 1
llama_context: reserving graph for n_tokens = 512, n_seqs = 1
llama_context:        CPU compute buffer size =   506.00 MiB
llama_context: graph nodes  = 987
llama_context: graph splits = 1
CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 
Model metadata: {'tokenizer.chat_template': "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.bos_token_id': '2', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '24576', 'tokenizer.ggml.add_bos_token': 'true', 'gemma.attention.head_count': '16', 'general.name': 'gemma-1.1-7b-it', 'gemma.context_length': '8192', 'gemma.embedding_length': '3072', 'gemma.block_count': '28', 'gemma.attention.head_count_kv': '16', 'gemma.attention.key_length': '256', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'gemma.attention.value_length': '256', 'general.file_type': '15'}
Available chat formats from metadata: chat_template.default
Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '
' + message['content'] | trim + '<end_of_turn>
' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model
'}}{% endif %}
Using chat eos_token: <eos>
Using chat bos_token: <bos>
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[96:39:29.463675184] [63989]  INFO Camera camera_manager.cpp:326 libcamera v0.5.0+59-d83ff0a4
[96:39:29.683626018] [63989]  INFO Camera camera.cpp:1205 configuring streams: (0) 640x480-MJPEG
ðŸ”´ Live feed started. Press 's' to analyze the scene for 'open a parcel'. Press 'q' to quit.
ðŸ§  Detected objects: ['scissors', 'chair', 'cup', 'mouse', 'knife', 'chair']
llama_perf_context_print:        load time =    9104.26 ms
llama_perf_context_print: prompt eval time =    9104.01 ms /   119 tokens (   76.50 ms per token,    13.07 tokens per second)
llama_perf_context_print:        eval time =   52872.53 ms /   163 runs   (  324.37 ms per token,     3.08 tokens per second)
llama_perf_context_print:       total time =   62090.63 ms /   282 tokens

ðŸ“¨ Prompt:
 Step 1:
- Scissors: Scissors can be used to cut through the packaging of the parcel.
- Chair: A chair is not useful for opening a parcel.
- Cup: A cup is not useful for opening a parcel.
- Mouse: A mouse is not useful for opening a parcel.
- Knife: A knife can be used to cut through the parcel's packaging.

Step 2:
- Scissors and knife are both useful for cutting through the packaging, but the knife is sharper and more suitable for this task.

Step 3:
- The knife is the highest-priority object because it is sharper and more suitable for cutting through the packaging compared to the scissors.

So, the best object to open the parcel is the knife.
llama_perf_context_print:        load time =   59058.93 ms
llama_perf_context_print: prompt eval time =   59058.64 ms /   159 tokens (  371.44 ms per token,     2.69 tokens per second)
llama_perf_context_print:        eval time =    6089.55 ms /     9 runs   (  676.62 ms per token,     1.48 tokens per second)
llama_perf_context_print:       total time =   65168.07 ms /   168 tokens

ðŸ“„ Generated Answer:
 

ðŸ§  LLM PERFORMANCE METRICS:
â€¢ Load Duration:          60.72 sec
â€¢ Total Duration:         65.17 sec
â€¢ Time to First Token:    59.06 sec
â€¢ Prompt Tokens:          159
â€¢ Generated Tokens:       1
â€¢ Prompt Eval Rate:       2.44 tokens/sec
â€¢ Generation Eval Rate:   0.02 tokens/sec


